---
title: Using GPT-4 turbo for Section 230 stance classification
author: Hauke Licht
date: 2024-06-07
format: 
  html:
    embed-resources: true
---

In this notebook, we take data analyzed in Gilardi et al. ([2023](https://doi.org/10.1073/pnas.2305016120)) to illustrate how to use GPT-4-turbo through the OpenAI chat completions API to classify stances in tweets.

**Note** &mdash; You can also use any other GPT model from OpenAI

## Setup

In the prompts and data folders, I use a common identifier that links the instruction files and the corresponding labeled data:

```{r}
TASK <- "gilardi_chatgpt_2023-section230_stance"
```

```{r}
#| warning: false
library(readr)
library(dplyr)
library(purrr)
library(openai)
stopifnot(Sys.getenv("OPENAI_API_KEY") != "")
# note: if you do not have an OpenAI Plus subscription, use gpt-3.5-turbo instead
MODEL = 'gpt-4-0125-preview'
library(metrica)
```

## Load the data

```{r}
#| message: false
base_path <- file.path('..', '..')
data_path <- file.path(base_path, 'data', 'gilardi_chatgpt_2023') 

fp <- file.path(data_path, paste0(TASK, ".csv"))
df <- read_csv(fp, col_types = "ccc") # <= read all three columns as character vectors
nrow(df)
```

```{r}
head(df, 2)
```

```{r}
table(df$label)
```

```{r}
df$text[1]
df$label[1]
```

## Define the instructions

We take the [original coding instructions](https://www.pnas.org/doi/suppl/10.1073/pnas.2305016120/suppl_file/pnas.2305016120.sapp.pdf) use by Gilardi *et al.* (2023) and slightly adapt them.

```{r}
prompts_path <- file.path(base_path, 'prompts') 
fp <- file.path(prompts_path, paste0(TASK, ".txt"))
instructions <- read_lines(fp)
instructions <- paste(instructions, collapse = "\n")
cat(instructions)
```

## Zero-shot classification

Zero-shot classification means to classif a user input by providing only instructions and no examples.

### simple example

```{r}
# construct the conversation history
convo <-  list(
  list(role = "system", content = instructions),
  list(role = "user", content = df$text[1])
)

response <- create_chat_completion(
  model = MODEL,
  messages = convo,
  # for reproducibility
  temperature = 0.0
)

classification <- response$choices$message.content[1]
classification
```

### automate

Typically we do not want to classify only one text but many.
In this case we need to iterate over them and send each of them to the LLM for classification -- one at a time.

To make the code more concise, let's first define a custom function that wraps the code used in the example above,
while abstracting away from concrete inputs (the specific tweet text and the specific system message) to general inputs (any tweet text and any system message):

**Note** &mdash; the think above the function definition is just documenting the function parameters using [roxygen2 syntax](https://r-pkgs.org/man.html).

```{r}
#' Classify the text of a single tweet with an OpenAI model using a custom system prompt
#'
#' @param text The to-be-classified tex (passed as user message to the model)
#' @param model The name of the model (see \url{https://platform.openai.com/docs/models})
#' @param system.message The system message used to instruct the model
#'
#' @return The classification of the text
classify_tweet <- function(text, model, system.message) {
  
  # clean the text 
  text <- gsub('\\s+', ' ', text)
  text <- trimws(text)
  
  # construct the conversation history
  convo <-  list(
    list(role = "system", content = system.message),
    list(role = "user", content = text)
  )

  # request a chat completion
  response <- create_chat_completion(
    model = model,
    messages = convo,
    # for reproducibility
    temperature = 0.0
  )
  
  classification <- response$choices$message.content[1]

  return(classification)
}
```

```{r}
set.seed(42)
samples <- df |> 
  group_by(label) |> 
  sample_n(25)
count(samples, label)
```

#### classify all examples

**Note** &mdash; running the below cell will take a couple of minutes
```{r}
# classify: apply custom classification function to all inputs
classifications <- map_chr(
  samples$text,
  classify_tweet,
  # additional parameters forwarded to `classify_tweet()`
  model=MODEL,
  system.message=instructions
)
```

```{r}
table(classifications)
```

Given that we had 25 examples per label class, we already see that there are some misclassifications.
Let's assess this more systematically.

#### Evaluate

Let's first just cross-tabulate the true and predicted labels

```{r}
table(
  "obs" = samples$label,
  "pred" = classifications
)
```
The LLM ha a negative bias (relative to the human-coded "ground truth"). 
This means that it assigns the negative level too often, resulting in the misclassification of truely "neutral" and "positive" examples.
Put more formally, its *precision* for the "negative" class is very low and, conversely, its *recall* for "neutral" and "positive" examples will be bad

So let's compute the precision, recall, and F1 metrics:

```{r}
precision(obs = samples$label, pred = classifications, atom = TRUE)
recall(obs = samples$label, pred = classifications, atom = TRUE)
fscore(obs = samples$label, pred = classifications, atom = TRUE)
```
We typically summarizes this information with the (unweighted) macro F1 score, which is just the average of class-wise F1 scores

```{r}
mean(fscore(obs = samples$label, pred = classifications, atom = TRUE)[[1]])
```
Judged by the macro F1, The overall performance is okayish.

**_Note_** &mdash; Gilardi et al. report an accuracy of ~0.7 (see panel A of their Figure 1).

## Few-shot classification

Few-shot classification means to classify a user input by providing examples in addition to instructions.

### simple example

Let's say we want to classify a single text:

```{r}
to_be_classified_text <- df$text[1]
to_be_classified_text
```

To demonstrate the desired classification behavior, we need **examples**, that is, pairs of texts and their "true" labels that allow the LLM to learn about the desired response behavior.

Let's start by selecting two examples per label class as examples, sampling from all but the to-be-classified text:

```{r}
# select two examples per label class, sampling from all but the to-be-classified text
set.seed(42)
examples <- df |> 
  filter(text != to_be_classified_text) |> 
  select(text, label) |> 
  group_by(label) |> 
  sample_n(2) |> 
  ungroup() |> 
  # reshuffle order
  sample_frac(1.0)
```

We need to convert this data frame into the list-of-lists format required to build the conversation history:

```{r}
examples <- examples |> 
  as.list() |> 
  transpose() |> 
  map(
    ~list(
      list(role = "user", content = .$text),
      list(role = "assistant", content = .$label)
    )
  ) |> 
  flatten()
head(examples, 2) 
```

Now, let's construct the conversation history.
The important thing to remember is that the examples must come after the instuctions but before the to-be-classified text.

```{r}
convo <- c(
  list(list(role = "system", content = instructions)),
  examples,
  list(list(role = "user", content = to_be_classified_text))
)

head(convo, 3)
tail(convo, 2)
```

```{r}
response <- create_chat_completion(
  model = MODEL,
  messages = convo,
  # for reproducibility
  temperature = 0.0
)

classification <- response$choices$message.content[1]
classification
```

### automate

The same logic applies as in the zero-shot case.
The only difference is that we need to add a function argument that takes the examples and adds them to the conversation history. 

```{r}
```
```{r}
#' Classify the text of a single tweet with an OpenAI model using a custom system prompt
#'
#' @param text The to-be-classified tex (passed as user message to the model)
#' @param examples list of lists of char message objects that have 'role' and 'content' elements
#' @param model The name of the model (see \url{https://platform.openai.com/docs/models})
#' @param system.message The system message used to instruct the model
#'
#' @note
#'    The `examples` are added after the system message and before the `text` is 
#'    provided as user inout in the last message of the conversation history
#'    
#' @return The classification of the text
fewshot_classify_tweet <- function(text, examples, model, system.message) {
  
  # validate the examples format
  stopifnot(
    "`examples` must be a list object" = is.list(examples), 
    "all elements of `examples` must be list ojects" = all(map_lgl(examples, is.list)), 
    "all elements of `examples` must have elements 'role' and 'content'" = all(map_lgl(map(examples, names), ~identical(., c("role", "content"))))
  )
  roles <- map_chr(examples, "role")
  stopifnot(
    "elements in `examples` must alternate between user and assistant messages" = all(roles[seq_along(roles) %% 2 == 1] == "user") & all(roles[seq_along(roles) %% 2 == 0] == "assistant")
  )

  # clean the text 
  text <- gsub('\\s+', ' ', text)
  text <- trimws(text)
  
  # construct the conversation history
  convo <-  c(
    list(list(role = "system", content = system.message)),
    examples,
    list(list(role = "user", content = text))
  )

  # request a chat completion
  response <- create_chat_completion(
    model = model,
    messages = convo,
    # for reproducibility
    temperature = 0.0
  )
  
  classification <- response$choices$message.content[1]

  return(classification)
}
```

Let's draw the to-be-classified samples:

```{r}
set.seed(42)
samples <- df |> 
  group_by(label) |> 
  sample_n(25)
count(samples, label)
```

Now we can select two examples per label class from the remaining tweets:

```{r}
set.seed(42)
examples <- df |> 
  filter(!status_id %in% samples$status_id) |> 
  select(text, label) |> 
  group_by(label) |> 
  sample_n(2) |> 
  ungroup() |> 
  # reshuffle order
  sample_frac(1.0)

# convert data frame into the list-of-lists format required to build the conversation history
examples <- examples |> 
  as.list() |> 
  transpose() |> 
  map(
    ~list(
      list(role = "user", content = .$text),
      list(role = "assistant", content = .$label)
    )
  ) |> 
  flatten()
```

#### classify all examples

**Note** &mdash; Iterating over the samples to classify them will take longer than previously because, in each iteration, the model also needs to process the text and label,s of examples in the conversation history.

```{r}
# classify: apply custom classification function to all inputs
fewshot_classifications <- map_chr(
  samples$text,
  fewshot_classify_tweet,
  examples=examples, # <== add examples here (same across to-be-classified texts)
  # additional parameters forwarded to `fewshot_classify_tweet()`
  model=MODEL,
  system.message=instructions
)
```

#### Evaluate

Let's first just cross-tabulate the true and predicted labels

```{r}
table(
  "obs" = samples$label,
  "pred" = fewshot_classifications
)
```
The LLM still has a negative bias (relative to the human-coded "ground truth") but its less pronounced than in the zero-shot application. 

Let's compute the precision, recall, and F1 metrics and compare them to the zero-shot results 

```{r}
list(
  "zeroshot" = precision(obs = samples$label, pred = classifications, atom = TRUE)[[1]],
  "fewshot" = precision(obs = samples$label, pred = fewshot_classifications, atom = TRUE)[[1]]
)
```
Because the negativity bias was reduced through fewshot learning, precision in classifying negative samples goes up.
But this comes at the cost of a reduced positivity bias (precision goes down).

```{r}
list(
  "zeroshot" = recall(obs = samples$label, pred = classifications, atom = TRUE)[[1]],
  "fewshot" = recall(obs = samples$label, pred = fewshot_classifications, atom = TRUE)[[1]]
)
```
But the recall increases in all but th "neutral" class.

Accordingly, we find improved F1 scores in all label classes:
```{r}
list(
  "zeroshot" = fscore(obs = samples$label, pred = classifications, atom = TRUE)[[1]],
  "fewshot" = fscore(obs = samples$label, pred = fewshot_classifications, atom = TRUE)[[1]]
)
```
This is reflected in an increase in the macro F1 score:

```{r}
c(
  "zeroshot" = mean(fscore(obs = samples$label, pred = classifications, atom = TRUE)[[1]]),
  "fewshot" = mean(fscore(obs = samples$label, pred = fewshot_classifications, atom = TRUE)[[1]])
)
```

**Note** &mdash; In this example we have just randomly sampled examples for few-shot prompting. But there are more sophisticated methods for selecting most-informative examples for a to-be-classified text from a set of human-labeled example texts.