---
title: Zero-shot text classification with OpenAI's GPT models
author: Hauke Licht
date: 2024-06-06
format: 
  html:
    embed-resources: true
---

This notebook illustrates how to use generative LLMs for zero-shot text classification.
We will use examples from the replication data of Gilardi et al. ([2023](https://www.pnas.org/doi/10.1073/pnas.2305016120)) because its one of the most often cited political science papers on evaluating ChatGPT for political text classification.

## Zero-shot text classification with LLMs

There are **thee steps** for performing zero-shot text classification:

1. define the instructions for the classification task
2. combine the task instruction and the to-be-classified text in a convsersation history
3. send these inputs to the model

### R Setup 

```{r}
library(openai)
stopifnot(Sys.getenv("OPENAI_API_KEY") != "")
```

## 1. Define the task

In this example, we adapt the instruction for one of the tweet classification tasks examined in Gilardi et al. ([2023](https://www.pnas.org/doi/10.1073/pnas.2305016120)) "ChatGPT outperforms crowd workers for text-annotation tasks"

- see [this README file](https://github.com/haukelicht/llm_text_coding_r/data/gilardi_chatgpt_2023/README.md) for a description of the data and tasks covered in the paper
- see [this file](https://github.com/haukelicht/llm_text_coding_r/data/gilardi_chatgpt_2023/instructions.md) for a copy of their original task instructions

```{r}
system_message <- "
Act as a text classifiction system.

Follow these instructions to classify the tweet's text:

1. Carefully read the text of the tweet, paying close attention to details.
2. Classify the tweet as either relevant or irrelevant

Classify the text into one of the given categories: \"Relevant\", \"Irrelevant\"

- Tweets should be coded as RELEVANT when they directly relate to content moderation, as defined above. This includes tweets that discuss: social media platformsâ€™ content moderation rules and practices, governmentsâ€™ regulation of online content moderation, and/or mild forms of content moderation like flagging.
- Tweets should be coded as IRRELEVANT if they do not refer to content moderation, as defined above, or if they are themselves examples of moderated content. This would include, for example, a Tweet by Donald Trump that Twitter has labeled as â€œdisputedâ€, a tweet claiming that something is false, or a tweet containing sensitive content. Such tweets might be subject to content moderation, but are not discussing content moderation. Therefore, they should be coded as irrelevant for our purposes.

Only include the selected category in your response and no further text.
"
cat(system_message)
```

# 2. Construct the conversation history

```{r}
# let's use an example tweet
tweet <- "@connybush Sorry hun, Ive removed the tags on IG d person handling my account thought you are my friend dats why u were tagged on both posts."

# clean the text 
# a) replace any white spaces, tabs and line breaks with a single white space
tweet <- gsub('\\s+', ' ', tweet)
# b) remove leading and trailing white spaces
tweet <- trimws(tweet)
```

```{r}
# construct the conversation history
convo <-  list(
  list(role = "system", content = system_message), # 1<== the task instructions go here
  list(role = "user", content = tweet)
)
```

## 3. Make the API Call (illustration with GPT 3.5)

```{r}
# request a chat completion
MODEL = 'gpt-3.5-turbo-0125'
response <- create_chat_completion(
  model = MODEL,
  messages = convo,
  # for reproducibility
  temperature = 0.0
)
```

### Parse the result

```{r}
classification <- response$choices$message.content[1]
classification
```

## _Advanced_: Iterate over several examples

Typically we do not want to classify only one text but many.
In this case we need to iterate over them and send each of them to the LLM for classification -- one at a time.

To make the code more concise, let's first define a custom function that wraps the code used in the example above,
while abstracting away from concrete inputs (the specific tweet text and the specific system message) to general inputs (any tweet text and any system message):

**Note** &mdash; the think above the function definition is just documenting the function parameters using [roxygen2 syntax](https://r-pkgs.org/man.html).

```{r}
#' Classify the text of a single tweet with an OpenAI model using a custom system prompt
#'
#' @param text The to-be-classified tex (passed as user message to the model)
#' @param model The name of the model (see \url{https://platform.openai.com/docs/models})
#' @param system.message The system message used to instruct the model
#'
#' @return The classification of the text
classify_tweet <- function(text, model, system.message) {
  
  # clean the text 
  text <- gsub('\\s+', ' ', text)
  text <- trimws(text)
  
  # construct the conversation history
  convo <-  list(
    list(role = "system", content = system.message),
    list(role = "user", content = paste0("'''", text, "'''"))
  )

  # request a chat completion
  response <- create_chat_completion(
    model = model,
    messages = convo,
    # for reproducibility
    temperature = 0.0 #<== note: we could make this a function argument, too
  )
  
  classification <- response$choices$message.content[1]

  return(classification)
}
```

No let's see how to apply this function to multiple texts one at a time.
But first, we need to define the texts we want to classify.
Here I just select five negative and five positive examples from the replication data:

```{r}
texts <- c(
    # negative examples ("irrelevant")
    "\"Turns out Mike Bloomberg is exactly what Elizabeth Warren needed to break through in the 2020 Democratic primary. And heâ€™s not just a foil for her on the campaign trail â€” this is something she believes in, and it shows.\" https://t.co/1SyaHXrZlO",
    "@blackhat___05 ye raha new user name change kiya kamine neðŸ˜¡ðŸ—¡ï¸ðŸ˜¡ðŸ—¡ï¸ðŸ˜¡ðŸ—¡ï¸ðŸ˜¡ karo abhi FNfollow reopt aur block",
    "The Kid!\n \nRETWEET for a chance at a @RawlingsSports baseball signed by Ken Griffey Jr. and tune in to #Junior tonight at 8pm ET/5pm PT on MLB Network.\n \nRules: https://t.co/MdkXLh1CdN | NoPurNec, US 18+, Ends 6/22 https://t.co/8Xw0HpHz2G",
    "TW / gore \n\nif you come across an account and want to block them, make sure to cover the bottom half of your screen. the gore is normally at the bottom of the screen. again, stay safe, and take precaution",
    "@Godlesswh_re Blocked.  Is this another Nick account?",
    # positive examples ("relevant")
    "Twitter we want you to suspend Marcon's account.\n#twitterSuspendMacronAccount #TwitterSuspendMarcon @verified @Twitter @TwitterSupport",
    "Twitter needs to permanently suspend @realDonaldTrump account.  Who's with me?",
    "Toei is one of the most active reporters of content on Youtube and everything runs through an auto filter. Today, Toei dropped a ridiculous volume of their own series onto an official Youtube channel and GOT BANNED AND REPORTED BY THEMSELVES, TOEI.",
    "Marsha Blackburn: We Are Looking at Antitrust Laws and Section 230 on Tech Censorship https://t.co/lsOWzD0Yri",
    "#Facebook has banned the iconic photograph of a #Soviet solider waving the #USSR flag over the #Reichstag in May 1945. The social network claims the image violates its community guidelines for dangerous people and organizations...\n\nMORE: https://t.co/arpDN9Ss0P https://t.co/KGtGwE4D5J"
)
```

Now we can 

1. take the vector of tweet texts (`texts`), 
2. iterate over it with `purrr::map_chr` (similar to base R's `lapply` or `vapply`), and 
3. pass each text value to our custom `classify_tweet()` function

**Note** &mdash; Because the model and the system message are constant across texts, we just pass them as additional arguments to `map_chr()` so that they are applied in each iteration.

```{r}
library(purrr)
classifications <- map_chr(
  texts,  # <== tweet texts to be classified 
  classify_tweet, # <== custom function performing API call
  # additional parameters forwarded to `classify_tweet()`
  model=MODEL, # <== model name (defined above)
  system.message=system_message # <== system message (defined above)
)
```

Let's inspect the resulting vector of classifications:

```{r}
classifications
```

This looks already quite good as there are only a few misclassifications! ðŸ¥³

**Note** &mdash; see the 'evaluation_metrics.qmd' notebook for details about how to compute numeric metrics that allow quantifying classification performance.

## Limitations 

### Multiple inputs per request

In theory, we can also combine several texts in one user message.
For this, we first need to update the system message to tell the model that it will receive multiple texts in separate lines in the user input:

```{r}
system_message <- "
Act as a text classification system.

Each line in the input is a separate tweet.

For each tweet in the input, follow these instructions to classify the tweet's text:

1. Carefully read the text of the tweet, paying close attention to details.
2. Classify the tweet as either relevant or irrelevant

Classify each tweet into one of the given categories: \"Relevant\", \"Irrelevant\"
 
- Tweets should be coded as RELEVANT when they directly relate to content moderation, as defined above. This includes tweets that discuss: social media platformsâ€™ content moderation rules and practices, governmentsâ€™ regulation of online content moderation, and/or mild forms of content moderation like flagging.
- Tweets should be coded as IRRELEVANT if they do not refer to content moderation, as defined above, or if they are themselves examples of moderated content. This would include, for example, a Tweet by Donald Trump that Twitter has labeled as â€œdisputedâ€, a tweet claiming that something is false, or a tweet containing sensitive content. Such tweets might be subject to content moderation, but are not discussing content moderation. Therefore, they should be coded as irrelevant for our purposes.

Only include the selected category in your response and no further text. Seperate the classifications of individual tweet by newline characters.
"
cat(system_message)
```

Further, we need to adapt our classification function to concateneate texts with new line characters and split the response message at the new line character:

```{r}
classify_tweets <- function(texts, model, system.message) {

  # clean the texts
  texts <- gsub('\\s+', ' ', texts)
  texts <- trimws(texts)
  # concatenate
  texts <- paste0("'''", texts, "'''", collapse = "\n")  
  texts <- paste0(texts, "\n")
  
  # construct the conversation history
  convo <-  list(
    list(role = "system", content = system.message),
    list(role = "user", content = texts)
  )

  # request a chat completion
  response <- create_chat_completion(
    model = model,
    messages = convo,
    # for reproducibility
    temperature = 0.0
  )
  
  classifications <- response$choices$message.content[1]
  classifications <- strsplit(classifications, "\n+")[[1]]

  return(classifications)
}
```

We can use this function with our ten example tweets:

```{r}
response <- classify_tweets(texts, model=MODEL, system.message=system_message)
response
```

**The problem** is that with this approach, each text's classification will depend on the order of texts in the input.
I demonstrate this below by randomly reshuffling texts orders and classifying them again:

```{r}
idxs <- seq_along(texts)
set.seed(42)
classifications <- list()
for (iter in 1:5) {
  message("\b\rIteration: ", iter)
  tmp <- sample(idxs)
  outputs <- classify_tweets(texts[tmp], model=MODEL, system.message=system_message)
  classifications[[iter]] <- outputs[order(tmp)]
}
```

```{r}
classifications <- tolower(do.call(cbind, classifications))
classifications
```

For example, if we look at the classifications of the first text (in the first row), we see that it has been coded irrelevant in 4 out of 5 times and relevant in 1 out of 5 times.
(Note also that in some cases, the model returned fewer classifications than input texts, resulting in `NA` values.)
We can quantify the level of disagreement in repeated classifications of the same text by looking at the [entropy](https://medium.com/street-science/entropy-how-to-actually-measure-uncertainty-dd1bd7d56235) of classifications of each text, which measures their uncertainty: 

```{r}
entropy <- function(x) {
  tab <- table(x)
  probs <- tab/sum(tab)
  -sum(probs * log2(probs))
}
apply(classifications, 1, entropy)
```

**The practical implication** of this is that if you can afford it, you should always just pass one text at a time to the LLM.


### Reproducibility

The classification results you get when calling the same OpenAI GPT model on the same text multiple times can vary.
This is because their system is not deterministic.
To mitigate this issue, yu should always set the [*temperature* parameter](https://platform.openai.com/docs/api-reference/chat/create#chat-create-temperature) to 0 (as already done above!).

Moreover, OpenAI has introduced the [*seed* parameter](https://platform.openai.com/docs/api-reference/chat/create#chat-create-seed) that further reduces randomness in the text generation process.
**The problem** is that the current version of the `openai` R package (0.4.1) does not allow us to pass this parameter when calling `create_chat_completion()` (I last cheched this on June 7, 2024).
